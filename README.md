# Job Application Task: Predictive Modeling with Natural Language Descriptions

This repository contains the framework for the Applied Machine Learning job application task where applicants build predictive models to predict future values `y_{n+1:n+k}` given observed data and natural language descriptions.

## Task Overview

Implement a stateless web-based API that can make predictions for future y values from a single request payload containing:
   - Observed data: `x_observed` (n×d), `y_observed` (n×1)
   - Future inputs: `x_predict` (k×d)
   - Natural language description: `t`
   - Dimensions: `n`, `k`, `d`

We provide a small but diverse **development dataset** to help you prototype:

* `development_payloads.json` – 20 self-contained payloads, each comprising `x_observed`, `y_observed`, `x_predict`, natural-language description `t`, and dimensions `n, k, d`.
* `development_y_true.json` – the corresponding ground-truth `y` values for the `x_predict` arrays.

These data cover a variety of functional relationships (linear, quadratic, periodic, etc.).

 **Do not hard-code anything specific to these 20 cases** – the final evaluation will use a much larger, unseen set of ~40 payloads drawn from the same generative process.

The goal is to minimise RMSE in predicting `y` for each payload while remaining reasonably efficient. 
API calls should take less than 60 seconds to return on hardware comparable to an AWS [c6i.xlarge instance](https://instances.vantage.sh/aws/ec2/c6i.xlarge), including calls to third-party APIs. We encourage faster solutions if they do not sacrifice predictive accuracy.

You can use any tools you like and any code you wrote before the task began, but please do not seek help from other people. Please document the tools, libraries, and other resources you use. For the API we encourage you to use Flask; if you take an alternative approach, please make it easy to deploy and document its advantages over Flask.

We anticipate this task will not take more than a regular working day to complete to a reasonable standard. If you find yourself pressed for time, we welcome you to document what you would have done if you'd allocated a full working week to it.

## Third-party API calls, e.g., LLMs

Your API service may call third-party APIs, notably including LLMs. We must be able to run your service without incurring additional costs or undertaking major work to set things up. Therefore, please use free-tier services that are straightforward to use, e.g., services where obtaining an API key is straightforward. We recommend looking into [Groq Cloud](https://console.groq.com/docs/overview). If you would like to use another service or have questions, please get in touch.

Please do not expose your own API keys to us, and please document any necessary setup.

## Mathematical Formulation

- **Input features**: `x` is a 2D matrix of shape `(n+k, d)` where `d` is the feature dimensionality
- **Target values**: `y` is a 1D vector of shape `(n+k,)`
- **Description**: `t` is a natural language sentence describing the relationship between x and y
- **Observed points**: First `n` points are observed during prediction
- **Prediction target**: Predict `y_{n+1:n+k}` given `x_{n+1:n+k}`, `x_{1:n}`, `y_{1:n}`, and `t`

The true generative distribution for $y=f(x)$ will not be disclosed, but are all parametric functions with additive Gaussian noise. The text is generated by human annotators who have seen both training and test points, but are not provided the true underlying function.

The range of x for observed points is -4 to 4. The range of x for points to predict is -5 to 5. You may assume that $d=3$.

## Repository Structure

The core files that are provided for the task:

```
task/
├── test_stateless_api.py       # Test script to verify your API accepts and returns payloads with the correct syntax
├── development_payloads.json   # 20 training payloads you may use for development
├── development_y_true.json     # Ground-truth y values for those 20 payloads
├── requirements.txt            # Python dependencies
└── README.md                   # This file
```

## Steps for Applicants

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

You may install additional dependencies; please update your `requirements.txt` accordingly.

### 2. Implement API

Implement an API such that running

```bash
python applicant_api.py
```

starts a stateless Flask API on `http://localhost:5000` with the following endpoints:

- `POST /predict` - Make stateless predictions
- `GET /health` - Health check

### 3. Verify your API locally

We provide `test_stateless_api.py`, a **self-contained smoke-test** for your service. Once your server is running on `localhost:5000`, execute:

```bash
python test_stateless_api.py
```

The script performs three checks:

1. `GET /health` returns HTTP 200 and `{"status": "ok"}`
2. `POST /predict` on a *valid* payload returns HTTP 200 and `{"status": "success", "predictions": [...]}` with the correct number of predictions
3. `POST /predict` on an *invalid* payload (missing required fields) returns HTTP 400 and an informative error JSON

All three must pass before submission.

### 4. OPTIONAL/BONUS: Implement a front-end

If you find yourself with extra time and energy and do not see easy opportunities to improve your core code, we welcome you to implement a minimal front-end that connects to the API, visualises points and predictions, and allows uploading of training data.

### 5. Document what you did and why

Please provide a 1-page `API_README.md` file describing:

1. Your general approach and what assumptions you made.
2. What tools you used, including GenAI development tools like GitHub Copilot or Cursor.
3. A summary of how your code is structured and how to use it.
4. Any other information you take to be important.

### 6. Package and send to us -- submission instructions

Please package your code and documentation so that it can be started with a single command on a fresh Linux machine (Docker is highly encouraged but optional).

* If you implement a front-end, please include it in the same repository as the back-end. The front-end should listen on port 5173.
* Any front-end should be able to run using only dependencies that can be installed using an install script or one-liner you provide, and should be able to run using a one-liner or provided shell script.



## API Specification

### Stateless Prediction Endpoint: `POST /predict`

**Request:**
```json
{
  "x_observed": [[0.5, -1.2, 0.8], ...],
  "y_observed": [2.1, 1.8, -0.5, ...],
  "x_predict": [[0.7, -1.1, 0.2], ...],
  "t": "The output y looks approximately quadratic in x2, but it flattens out for extreme values of x2",
  "n": 20,
  "k": 25,
  "d": 3
}
```

**Response:**
```json
{
  "status": "success", 
  "predictions": [1.2, 2.9, ...]
}
```

### Error handling & standard error codes

Your API **must** return informative JSON for error cases so that our evaluation framework can distinguish between model failures and protocol violations. We will specifically test for the following scenarios:

| Scenario | Expected HTTP status | Example JSON body |
| -------- | ------------------- | ----------------- |
| Missing required field (e.g. `x_observed`) | `400 Bad Request` | `{ "status": "error", "message": "Missing required field: 'x_observed'" }` |
| Shape or dimension mismatch | `400 Bad Request` | `{ "status": "error", "message": "Invalid input: x_observed shape (6, 5) doesn't match expected (6, 4)" }` |
| Internal prediction failure | `500 Internal Server Error` | `{ "status": "error", "message": "Linear regression failed" }` |
| Request took longer than timeout window | `408 Request Timeout` | `{ "status": "error", "message": "Request timeout" }` |

Feel free to extend this list but please **keep the above codes intact**.

### Health Check Endpoint: `GET /health`

**Request:**

```
GET /health HTTP/1.1
Host: localhost:5000
```

**Response:**

```json
{
  "status": "ok"
}
```

## Data Format

- **x_observed**: Observed feature matrix of shape `(n, d)` as nested lists
- **y_observed**: Observed target vector of shape `(n,)` as list
- **x_predict**: Future feature matrix of shape `(k, d)` as nested lists
- **t**: Natural language description as string
- **n**: Number of observed points (integer)
- **k**: Number of points to predict (integer)
- **d**: Feature dimensionality (integer)

## Evaluation

Our evaluation will have two parts. The first is a quantitative assessment in which models are evaluated on **≈40 held-out payloads** that you have not seen:

- **Primary metric**: RMSE (Root Mean Square Error) - lower is better
- **Secondary metrics**: average response time - lower is better

Our evaluation framework:

1. Checks API health and response correctness to well-formed and ill-formed inputs
2. Makes multiple (60–200) stateless prediction requests on test data — please ensure that these calls will not exceed daily rate limits
3. Computes RMSE between predicted and true values, as well as response latencies
4. Reports summary statistics on RMSEs and latencies

The second part is a human assessment of your documentation, code, and design choices. Important considerations include justifying non-conventional decisions, clarity, ease of deployment and use.

## Implementation Tips

1. **Stateless design**: No server-side state - each request is independent
2. **Use observed data**: Using `x_observed` and `y_observed` is essential
3. **Use descriptions**: This can improve performance in the face of sparse data; remember that the descriptions are informed by the true values of `y` for `x_observed`
4. **Training data**: Use the provided training data however you like; we do not guarantee that all test points will be drawn from the same distribution
5. **Error handling**: Implement robust error handling for malformed requests
6. **Performance**: Optimize for low response times while maintaining accuracy
7. **Pragmatism**: There are many ways to go about the task, some of which are very time or resource intensive; try to find a good balance


## Additional notes

- APIs must run on `localhost:5000` for evaluation
- Response timeout is 90 seconds per prediction request
- All data uses JSON serialization with Python lists (not NumPy arrays)

## Support

For questions about the task or technical issues, please refer to the development payloads and the examples shown above. If anything is unclear, please contact us. 
